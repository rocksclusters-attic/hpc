<section id="environment_modules" xreflabel="Environment Modules for OpenMPI">
	<title>Environment Modules for OpenMPI</title>

<para>
As of Rocks 5.5 and 6.0 (Mamba), Environment Modules are utilized to control
MPI path names.   By default the rocks-openmpi module is loaded and
is openmpi compiled with gnu compiler and the ethernet device. 
</para>

<itemizedlist>

<listitem>
	<para>
	To see the currently loaded modules:
	</para>

<screen>
% module list
Currently Loaded Modulefiles:
  1) rocks-openmpi
%
</screen>
</listitem>

<listitem>
	<para>
	To see available modules:
	</para>

<screen>
% module avail
------------------------ /usr/share/Modules/modulefiles ------------------------
dot           module-info   null          use.own
module-cvs    modules       rocks-openmpi

------------------------ /usr/share/Modules/modulefiles ------------------------
dot           module-info   null          use.own
module-cvs    modules       rocks-openmpi
%
</screen>

</listitem>

<listitem>
	<para>
	To NOT load the Rocks default module Definition. Set the environment
	variable ROCKS_MODULE_USER_DEF to a non-zero string. 
	</para>

<screen>
export ROCKS_USER_MODULE_DEF=True
</screen>
<tip>
<para> if modules are already loaded, then ROCKS_USER_MODULE_DEF will not
unload already loaded modules.  If you do not want the Rocks default then set the above definition in your $HOME/.bashrc or $HOME/.cshrc  files
</para>
</tip>
</listitem>
</itemizedlist>

</section>

<section id="using_openmpi" xreflabel="Using mpirun from OpenMPI">
	<title>Using mpirun from OpenMPI</title>

<para>
To interactively launch a test OpenMPI program on two processors:
</para>

<itemizedlist>

<listitem>
	<para>
	Create a file in your home directory named
	<filename>machines</filename>,
	and put two entries in it, such as:
	</para>

<screen>
compute-0-0
compute-0-1
</screen>
</listitem>

<listitem>
	<para>
	Now launch the job from the frontend:
	</para>

<screen>
$ ssh-agent $SHELL
$ ssh-add
$ /opt/openmpi/bin/mpirun -np 2 -machinefile machines /opt/mpi-tests/bin/mpi-ring 
</screen>

<para>
<caution>
	<para>
	You must run MPI programs as a regular user (that is, not root).
	</para>

	<para>
	If you don't have a user account on the cluster, create one
	for yourself, and propogate the information to the compute nodes with:

<screen>
# useradd <emphasis>username</emphasis>
# rocks sync users
</screen>

	</para>
</caution>
</para>

</listitem>
</itemizedlist>

</section>


<section id="using-mpirun-ethernet" xreflabel="Using mpirun from MPICH">
	<title> Using mpirun from MPICH</title>

<para>
To interactively launch a test MPICH program on two processors:
</para>

<itemizedlist>

<listitem>
	<para>
	Create a file in your home directory named
	<filename>machines</filename>,
	and put two entries in it, such as:
	</para>

<screen>
compute-0-0
compute-0-1
</screen>
</listitem>

<listitem>
	<para>
	Compile a test program using the MPICH environment:
	</para>

<screen>
$ cd $HOME
$ mkdir mpich-test
$ cd mpich-test
$ cp /opt/mpi-tests/src/mpi-ring.c .
$ /opt/mpich/gnu/bin/mpicc -o mpi-ring mpi-ring.c -lm
</screen>
</listitem>

<listitem>
	<para>
	Now launch the job from the frontend:
	</para>

<screen>
$ ssh-agent $SHELL
$ ssh-add
$ /opt/mpich/gnu/bin/mpirun -nolocal -np 2 -machinefile $HOME/machines \
	$HOME/mpich-test/mpi-ring
</screen>

<para>
<caution>
	<para>
	You must run MPI programs as a regular user (that is, not root).
	</para>

	<para>
	If you don't have a user account on the cluster, create one
	for yourself, and propogate the information to the compute nodes with:

<screen>
# useradd <emphasis>username</emphasis>
# rocks sync users
</screen>

	</para>
</caution>
</para>

</listitem>
</itemizedlist>

</section>

