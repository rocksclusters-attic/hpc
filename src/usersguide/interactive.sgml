<section id="using_openmpi" xreflabel="Using mpirun from OpenMPI">
	<title>Using mpirun from OpenMPI</title>

<para>
To interactively launch a test OpenMPI program on two processors:
</para>

<itemizedlist>

<listitem>
	<para>
	Create a file in your home directory named
	<filename>machines</filename>,
	and put two entries in it, such as:
	</para>

<screen>
compute-0-0
compute-0-1
</screen>
</listitem>

<listitem>
	<para>
	Now launch the job from the frontend:
	</para>

<screen>
$ ssh-agent $SHELL
$ ssh-add
$ /opt/openmpi/bin/mpirun -np 2 -machinefile machines /opt/mpi-tests/bin/mpi-ring 
</screen>

<para>
<caution>
	<para>
	You must run MPI programs as a regular user (that is, not root).
	</para>

	<para>
	If you don't have a user account on the cluster, create one
	for yourself, and propogate the information to the compute nodes with:

<screen>
# useradd <emphasis>username</emphasis>
# rocks sync users
</screen>

	</para>
</caution>
</para>

</listitem>
</itemizedlist>

</section>


<section id="using-mpirun-ethernet" xreflabel="Using mpirun from MPICH">
	<title> Using mpirun from MPICH</title>

<para>
To interactively launch a test MPICH program on two processors:
</para>

<itemizedlist>

<listitem>
	<para>
	Create a file in your home directory named
	<filename>machines</filename>,
	and put two entries in it, such as:
	</para>

<screen>
compute-0-0
compute-0-1
</screen>
</listitem>

<listitem>
	<para>
	Compile a test program using the MPICH environment:
	</para>

<screen>
$ cd $HOME
$ mkdir mpich-test
$ cd mpich-test
$ cp /opt/mpi-tests/src/mpi-ring.c .
$ /opt/mpich/gnu/bin/mpicc -o mpi-ring mpi-ring.c -lm
</screen>
</listitem>

<listitem>
	<para>
	Now launch the job from the frontend:
	</para>

<screen>
$ ssh-agent $SHELL
$ ssh-add
$ /opt/mpich/gnu/bin/mpirun -nolocal -np 2 -machinefile $HOME/machines \
	$HOME/mpich-test/mpi-ring
</screen>

<para>
<caution>
	<para>
	You must run MPI programs as a regular user (that is, not root).
	</para>

	<para>
	If you don't have a user account on the cluster, create one
	for yourself, and propogate the information to the compute nodes with:

<screen>
# useradd <emphasis>username</emphasis>
# rocks sync users
</screen>

	</para>
</caution>
</para>

</listitem>
</itemizedlist>

</section>


<section id="cluster-fork">
<title>Cluster-Fork</title>

<para>
Cluster-Fork runs a command on compute nodes of your cluster.
</para>

<para>
Often we want to execute parallel jobs consisting of standard UNIX commands. By "parallel" we mean the same command runs on multiple nodes of the cluster. We use these simple parallel jobs to move files, to run small tests, and to perform various administrative tasks.
</para>

<para>
Rocks provides a simple tool for this purpose called <literal>cluster-fork</literal>. For example, to list all your processes on the compute nodes of the cluster:
</para>

<screen>
$ cluster-fork ps -U$USER
</screen>

<para>
By default, cluster-fork uses a simple series of ssh connections to launch the task serially on every compute node in the cluster. Cluster-fork is smart enough to ignore dead nodes. Usually the job is "blocking": cluster-fork waits for the job to start on one node before moving to the next. By using the <literal>--bg</literal> flag you can instruct cluster-fork to start the jobs in the background. This corresponds to the "-f" ssh flag.
</para>

<screen>
$ cluster-fork --bg hostname
</screen>

<para>
Often you wish to name the nodes your job is started on. This can be done by using an SQL statement or by specifying the nodes using a special shorthand.
</para>

<para>
The first method of naming nodes uses the SQL database on the frontend. We need an SQL statement that returns a column of node names. For example, to run a command on compute nodes in the first rack of your cluster exectute:
</para>

<screen>
$ cluster-fork --query="select name from nodes where name like 'compute-1-%'" [cmd]
</screen>

<para>
The next method of requires us to explicitly name each node. When launching a job on many nodes of a large cluster this often becomes cumbersome. We provide a special shorthand to help with this task. This shorthand, borrowed from the MPD job launcher, allows us to specify large ranges of nodes quickly and concisely.
</para>

<para>
The shorthand is based on similarly-named nodes and uses the <literal>--nodes</literal> option. To specify a node range <emphasis>compute-0-0 compute-0-1 compute-0-2</emphasis>, we write <emphasis>--nodes=compute-0-%d:0-2</emphasis>. This scheme works best when the names share a common prefix, and the variables between names are numeric. Rocks compute nodes are named with such a convention.
</para>

<para>
Other shorthand examples:
</para>

<itemizedlist>
<listitem>
<para>
Discontinuous ranges:
</para>
<para>
<emphasis>compute-0-%d:0,2-3</emphasis> -->
<literal>compute-0-0 compute-0-2 compute-0-3</literal>
</para>
</listitem>

<listitem>
<para>
Multiple elements:
</para>
<para>
<emphasis>compute-0-%d:0-1 compute-1-%d:0-1</emphasis> -->
<literal>compute-0-0 compute-0-1 compute-1-0 compute-1-1</literal>
</para>
</listitem>

<listitem>
<para>
Factoring out duplicates:
</para>
<para>
<emphasis>2*compute-0-%d:0-1 compute-0-%d:2-2</emphasis> -->
<literal>compute-0-0 compute-0-0 compute-0-1 compute-0-1 compute-0-2</literal>
</para>
</listitem>
</itemizedlist>

<screen>
$ cluster-fork --nodes="compute-2-%d:0-32 compute-3-%d:0-32" ps -U$USER
</screen>

<para>
The previous example lists the processes for the current user on 64 nodes in racks two and three.
</para>

</section>

